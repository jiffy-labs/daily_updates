1. Created first interaction of the POC for decoding - https://github.com/jiffy-labs/jiffyscan-frontend/tree/578e0cd1bd72a364e064fa37c5d38a78fef39a2f/src/views/userOpMempool
  1. *TODO: remove change in the layout of the page. It is distracting.*
  2. *TODO: show seconds in "x ago" mode
  3. *TODO: research the average time taken by bundlers to submit it on the chain.
  4. *TODO: try out alchemy's and Stackup's bundler, to see what data is being stored in their mempool.*

2. Spoke with the fullstack engineer(Nilesh) about the Twitter issue where error's are thrown in the console, after each log-in redirection happens to the home page.
  1. *Note: It is extremely hard to debug as to what is going wrong since this issue is happening only in prod.* 
  2. *Next Step: Our hypothesis as to why the Twitter log-in is breaking only in prod has to do with DNS redirection, since only in production do we have two endpoints for callback (`www.jiffyscan.xyz` and `jiffyscan.xyz`).
    Removing any one of the redirection callback makes the Twitter log-in less stable. We need to figure out a different way to fix the bug.*
  3. *Asked resource to research for similar issues for 1-2hrs and if there is nothing, to make a post in the developer forum about this issue.*

  3. Brushed up on OKR book and also read different ways engineering teams create their OKR's.
     1. Created a list of OKR's for the engineering team - https://www.notion.so/adityaagarwal/OKR-s-c12aa05810ac4d83852cf5eb9d236ca0 
     2. *TODO: add OKR's to sugar*

  4. Spoke with DevOps engineer as to the limitations of Grafana and Kibana. 
    1. As per our current understanding, grafana's query layer is custom-built for elastic search and lacks functionality to do certain operations such as pick a single data point and creating a graph out of it.
       However the required functionality to work with elastic search exists in kibana. 
    2. The suggestion is to use kibana since kibana is the default dashboarding layer for elastic search data points and is better built for data operations on elastic search.
    3. He has tried using elastic custom indices ( these are computed on the go ) but was not able to get grafana to show network latency graphs using said index. 
    4. As of now,  asked him to see if there is any possible way to show "exact count" data in the grafana graph since that is what is needed to show network latency in elastic. 
    5. *TODO: Wait till tomorrow evening for his update on the research, depending on what he finds about queries, we might switch to Kibana for dashboarding ES.* 
    
    
  
  
